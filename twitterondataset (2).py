# -*- coding: utf-8 -*-
"""TwitterOnDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14nYfH83AgnkFdB3aeQFR59Tc-AhAwMg8

**Uploading the Dataset in colab library.**
"""

from pandas import DataFrame
from google.colab import files
uploaded = files.upload()

"""**Reading the uploaded dataset into a variable using pandas.**"""

import pandas
df = pandas.read_csv('train.csv', encoding = "ISO-8859-1")
df=df[20000:25000]

"""***Importing the libraries we need. ***

Download the set of english stopwords which should be removed from the tweets.
WordCloud is imported to form wordcloud of positive and negative words.
"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from wordcloud import WordCloud,STOPWORDS

"""**Cleaning of the tweets!**

Firstly remove the punctuations used in the tweets.Make use of regular expression to do it.
"""

import re
import string
def remove_punct(text):
    text  = "".join([char for char in text if char not in string.punctuation])
    text = re.sub('[0-9]+', '', text)
    return text

df['Tweet_punct'] = df['SentimentText'].apply(lambda x: remove_punct(x))
del df['SentimentText']
df.head()

"""Then tokenize the tweets. Basically you need to break the tweets into words."""

def tokenization(text):
    text = re.split('\W+', text)
    return text

df['Tweet_tokenized'] = df['Tweet_punct'].apply(lambda x: tokenization(x.lower()))
del df['Tweet_punct']
df.head()

"""Take the set of english stopwords into a variable."""

stopword = nltk.corpus.stopwords.words('english')

"""Using of the set of stopwords,check the tweets. If stopwords are present remove those stopwords.For checking make use of the tokenized tweets."""

def remove_stopwords(text):
    text = [word for word in text if word not in stopword]
    return text
    
df['Tweet_nonstop'] = df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))
del df['Tweet_tokenized']
df.head()

"""Now stemming needs to be done.Stemming is the process of producing morphological variants of a root/base word. A stemming algorithm reduces the words “chocolates”, “chocolatey”, “choco” to the root word, “chocolate” and “retrieval”, “retrieved”, “retrieves” reduce to the stem “retrieve”."""

ps = nltk.PorterStemmer()

def stemming(text):
    text = [ps.stem(word) for word in text]
    return text

df['Tweet_stemmed'] = df['Tweet_nonstop'].apply(lambda x: stemming(x))
del df['Tweet_nonstop']
df.head()

"""Here I just renamed the "tweet_stemmed" column to "SentimentText" for my convenience."""

df.columns=["ItemID","Sentiment","SentimentText"]
df.head()

"""**Appending the tweets and polarity into a list.**

Now iterate each row of the dataset and store the sentiment text ,that is basically the tweet, and its polarity in a list.
"""

tweets = []
for  index,row in df.iterrows():
   tweets.append((row.SentimentText,row.Sentiment))
print(tweets)

"""**Building the vocabulary and Matching tweets against the vocabulary.**

A vocabulary in Natural Language Processing includes all the words resident in the Training set we have.This is just creating a list of all_words we have in the Training set, breaking it into word features. Those word_features are basically a list of distinct words, each of which has its frequency as a key.
After this each word of the tweet in hand is matched with the words in the vocabulary.
"""

def buildVocabulary(data):
    all_words = []
    
    for (words, sentiment) in data:
        all_words.extend(words)

    wordlist = nltk.FreqDist(all_words)
    word_features = wordlist.keys()
    
    return word_features

w_features = buildVocabulary(tweets)

def extract_features(document):
    document_words = set(document)
    features = {}
    for word in w_features:
        features['contains(%s)' % word] = (word in document_words)
    return features

"""**Splitting the dataset into train and test set.**

Import the function for splitting data to train and test sets from sklearn and define the test size.
"""

from sklearn.model_selection import train_test_split 
train , test =train_test_split(tweets,test_size = 0.3)

"""Checking the datatype of tweet."""

print(type(tweets))

"""Datatype of tweets is list so it needs to converted into dataframe.Using pandas change it to dataframe."""

import pandas as pd
train=pd.DataFrame(train)
test=pd.DataFrame(test)

"""**Checking the data in train set.**"""

train.head()

"""**Renaming of columns.**

Since the name of columns have changed to 0/1 ,rename it to "SentimentText" and "Sentiment".
"""

train.columns=["SentimentText","Sentiment"]
test.columns=["SentimentText","Sentiment"]

"""Checking the clumns of test data set"""

test.columns

"""**Separating the positive and negative sentiment tweets.**

Separate the Positive and Negative tweets of the training set in order to easily visualize their contained words.
"""

train_pos = train[ train['Sentiment'] == 1]
train_pos = train_pos['SentimentText']
train_neg = train[ train['Sentiment'] == 0]
train_neg = train_neg['SentimentText']

print(train_pos)

print(train_neg)

"""As above see that, train_pos and train_neg both are lists of list, it is converted into a single list."""

tp=train_pos.to_list()
tp1=[]
for i in tp:
    for item in i:
        tp1.append(item)
tn=train_neg.to_list()
tn1=[]
for i in tn:
    for item in i:
        tn1.append(item)

"""**Checking the length of positive and negative list before removing common words.**"""

print(len(tp1))
print(len(tn1))

print(tp1)
print(tn1)

"""**Removing of common words in both the lists.**

From both the list of positive and negative words,common words are removed by comparing both the list.
"""

for i in tp1:
  if i in tn1:
    tp1.remove(i)
    tn1.remove(i)

"""**Checking the length after removing the common words.**"""

print(len(tp1))
print(len(tn1))

"""**Wordcloud formation**

From the list of positive and negative words wordclouds are formed.
Two wordclouds are formed one for positive words and another for negative words.
"""

import re
import matplotlib.pyplot as plt
def wordcloud_draw(data, color = 'black'):
    words = ' '.join(data)
    cleaned_word = " ".join([word for word in words.split()
                            if 'http' not in word
                            and not word.startswith('@')
                            and not word.startswith('#')
                            and not len(word)<3
                            ])
    wordcloud = WordCloud(stopwords=STOPWORDS,
                      background_color=color,
                      width=2500,
                      height=2000
                     ).generate(cleaned_word)
    plt.figure(1,figsize=(13, 13))
    plt.imshow(wordcloud)
    plt.axis('off')
    plt.show()
    
print("Positive words")
wordcloud_draw(tp1,'white')

print("Negative words")
wordcloud_draw(tn1)

"""**Training of the data.**

Using the nltk NaiveBayes Classifier classify the extracted tweet word features.
"""

from nltk.classify import SklearnClassifier
# Training the Naive Bayes classifier
training_set = nltk.classify.apply_features(extract_features,tweets)
classifier = nltk.NaiveBayesClassifier.train(training_set)

"""**Getting the actual sentiment of test set.**

Store the actual sentiment value of the test set into a list.
"""

t=test['Sentiment'].to_list()
print(t)

"""**Prediction of Polarity.**

Test the data from test set and append the result in a list.
"""

li=[]
for obj in test['SentimentText']: 
    for i in obj:
      res =  classifier.classify(extract_features(obj))
    li.append(res)
      
print(li)

"""**Importing library for Confusion Matrix,Classification Report and Accuracy Score.**"""

from sklearn.metrics import confusion_matrix 
from sklearn.metrics import classification_report 
from sklearn.metrics import accuracy_score

"""Print the confusion matrix,accuracy score and classification report by passing the list which contains the actual polarity value and list which contains the predicted polarity value."""

cm=confusion_matrix(t,li)
print(cm)
print ('Accuracy Score :',100*accuracy_score(t,li)) 
print(classification_report(t,li))

